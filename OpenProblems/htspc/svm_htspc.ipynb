{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "svm_1st_4.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "EK81V2Dmz7IA",
        "colab_type": "code",
        "outputId": "2d0c0586-8d3a-4aaf-85d6-92d3fefaad3f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 558
        }
      },
      "source": [
        "!pip install demoji\n",
        "!pip install emoji\n",
        "!pip install Unidecode\n",
        "import pandas as pd\n",
        "import sys\n",
        "import string  \n",
        "from sklearn.model_selection import train_test_split\n",
        "from nltk.stem import PorterStemmer\n",
        "from sklearn.metrics import classification_report\n",
        "import numpy as np\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "import cv2\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import matplotlib.image as mpimg\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, f1_score\n",
        "from sklearn.metrics import precision_score, recall_score, r2_score\n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "from sklearn import datasets, svm, metrics\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "from unidecode import unidecode\n",
        "from nltk.stem import PorterStemmer \n",
        "from nltk.tokenize import word_tokenize \n",
        "from keras.utils import to_categorical\n",
        "import demoji\n",
        "import emoji\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding,Bidirectional,Dense,Conv1D,Flatten,LSTM,GlobalMaxPooling1D,Dropout\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from sklearn.manifold import TSNE\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.optimizers import Adam\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from nltk.tokenize import WordPunctTokenizer\n",
        "from nltk.stem import WordNetLemmatizer \n",
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting demoji\n",
            "  Downloading https://files.pythonhosted.org/packages/da/0b/d008f26ebbfd86d21117267e627f2f7359c76e5ecbeba08d8f631f4092c4/demoji-0.2.1-py2.py3-none-any.whl\n",
            "Collecting colorama\n",
            "  Downloading https://files.pythonhosted.org/packages/c9/dc/45cdef1b4d119eb96316b3117e6d5708a08029992b2fee2c143c7a0a5cc5/colorama-0.4.3-py2.py3-none-any.whl\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from demoji) (46.1.3)\n",
            "Requirement already satisfied: requests<3.0.0 in /usr/local/lib/python3.6/dist-packages (from demoji) (2.23.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0->demoji) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0->demoji) (2.9)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0->demoji) (2020.4.5.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0->demoji) (3.0.4)\n",
            "Installing collected packages: colorama, demoji\n",
            "Successfully installed colorama-0.4.3 demoji-0.2.1\n",
            "Collecting emoji\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/40/8d/521be7f0091fe0f2ae690cc044faf43e3445e0ff33c574eae752dd7e39fa/emoji-0.5.4.tar.gz (43kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 3.9MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: emoji\n",
            "  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for emoji: filename=emoji-0.5.4-cp36-none-any.whl size=42176 sha256=2d51e09fbf60b30a4c3857bc7c9e8d63d97d8c795d64e784c4ff19849ab4f177\n",
            "  Stored in directory: /root/.cache/pip/wheels/2a/a9/0a/4f8e8cce8074232aba240caca3fade315bb49fac68808d1a9c\n",
            "Successfully built emoji\n",
            "Installing collected packages: emoji\n",
            "Successfully installed emoji-0.5.4\n",
            "Collecting Unidecode\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d0/42/d9edfed04228bacea2d824904cae367ee9efd05e6cce7ceaaedd0b0ad964/Unidecode-1.1.1-py2.py3-none-any.whl (238kB)\n",
            "\u001b[K     |████████████████████████████████| 245kB 9.1MB/s \n",
            "\u001b[?25hInstalling collected packages: Unidecode\n",
            "Successfully installed Unidecode-1.1.1\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dmh1P21_0FXM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class cleaning:\n",
        "  lemmatizer = WordNetLemmatizer()\n",
        "  tok = WordPunctTokenizer()\n",
        "  pat1 = r'@[A-Za-z0-9]+'\n",
        "  pat2 = r'https?://[A-Za-z0-9./]+'\n",
        "  combined_pat = r'|'.join((pat1, pat2))\n",
        "\n",
        "  def tweet_cleaner(self,text):\n",
        "      text = emoji.demojize(text)\n",
        "      soup = BeautifulSoup(text, 'lxml')\n",
        "      souped = soup.get_text()\n",
        "      stripped = re.sub(self.combined_pat, '', souped)\n",
        "      try:\n",
        "          clean = stripped.decode(\"utf-8-sig\").replace(u\"\\ufffd\", \"?\")\n",
        "      except:\n",
        "          clean = stripped\n",
        "      letters_only = re.sub(\"[^a-zA-Z]\", \" \", clean)\n",
        "      letters_only = re.sub('([A-Z][a-z]+)', r' \\1', re.sub('([A-Z]+)', r' \\1', letters_only))\n",
        "      lower_case = letters_only.lower()\n",
        "      words = self.tok.tokenize(lower_case)\n",
        "      wordss = []\n",
        "      for w in words: \n",
        "        wordss.append(self.lemmatizer.lemmatize(w)) \n",
        "      S = (\" \".join(wordss)).strip()\n",
        "      return S\n",
        "\n",
        "\n",
        "  def All_clean(self,Data):\n",
        "    test_result = []\n",
        "    for t in Data.text:\n",
        "      test_result.append(self.tweet_cleaner(t))\n",
        "    test_result = pd.DataFrame(test_result, columns=['text'])\n",
        "    # test_result = self.PreProcessing(test_result)\n",
        "    return test_result\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eOOMfyUh0IRD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.utils import shuffle\n",
        "df = pd.read_csv('/content/drive/My Drive/Data_4/1fe720be-90e4-4e06-9b52-9de93e0ea937_train.csv')\n",
        "cln = cleaning()\n",
        "# train_data = df.to_numpy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jza9yGbo2v1v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import html\n",
        "from emoji.unicode_codes import UNICODE_EMOJI\n",
        "label_data = df['labels']\n",
        "# pre_process(train_data)\n",
        "train_data = cln.All_clean(df)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Mj5Qxkd7BTq",
        "colab_type": "code",
        "outputId": "aba7fdf0-2d70-4700-9e9b-76414414d301",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 260
        }
      },
      "source": [
        "print(train_data)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                                                   text\n",
            "0     this is one of the worst time to be american b...\n",
            "1     how about the crowd in oval in today s au sv i...\n",
            "2     biden his son hunter took advantage of their p...\n",
            "3     etsy shop benedict donald so called president ...\n",
            "4     good build a wall around arkansas fuck trump f...\n",
            "...                                                 ...\n",
            "5261  should allow m dhoni to keep glove it is attac...\n",
            "5262  trump on avoiding movie pirating of course you...\n",
            "5263  i noticed recently jamie oliver s restaurant c...\n",
            "5264  team india geared up is okay what s on the glo...\n",
            "5265  is this the same piece of paper mc carthy used...\n",
            "\n",
            "[5266 rows x 1 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-HfTTFI60tBn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.svm import SVC\n",
        "class svm_classifier:\n",
        "  vectorizer = TfidfVectorizer()\n",
        "  clf = SVC(C=18, kernel='rbf')\n",
        "  def train(self,X,Y):\n",
        "    X=self.vectorizer.fit_transform(X['text'])\n",
        "    X_train,X_test,Y_train,Y_test = train_test_split(X,Y ,train_size = 0.80)\n",
        "    print(X_train.shape,' ',Y_train.shape,' ',X_test.shape,' ',Y_test.shape)\n",
        "    # X_train = X\n",
        "    # Y_train  = Y\n",
        "    hist=self.clf.fit(X_train,Y_train)\n",
        "    y_pred=self.clf.predict(X_test)\n",
        "    # print(y_pred)\n",
        "    y_true = Y_test.values\n",
        "    # print(y_true)\n",
        "    # acc = accuracy_score(y_true, y_pred)\n",
        "    # print(acc)\n",
        "    # print(f1_score(y_true, y_pred, average='macro'))\n",
        "    print(confusion_matrix(y_true, y_pred))\n",
        "  def test(self,test_data):\n",
        "    X_p = self.vectorizer.transform(test_data['text'])\n",
        "    y_pred_test = self.clf.predict(X_p)\n",
        "    files = pd.read_csv('/content/ff31f086-a067-4fb1-9735-597861b111f0_submission.csv')\n",
        "    files['labels'] = y_pred_test\n",
        "    # y = pd.DataFrame(y_pred_test, columns=['labels'])\n",
        "    files.to_csv('submission_1_svm.csv', header=True, index=False,row_index = false) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uciwFEg32V3L",
        "colab_type": "code",
        "outputId": "5c88c6c0-41a8-408d-ca6c-0b5c18e2959e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "cls_clf =  svm_classifier()\n",
        "# speech = train_data[:,0]\n",
        "print(train_data.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(5266, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bTKgDcPC7jBG",
        "colab_type": "code",
        "outputId": "c75d79e3-ca25-4b31-b260-e5a9af959ea5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "cls_clf.train(train_data,label_data)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(4212, 11656)   (4212,)   (1054, 11656)   (1054,)\n",
            "[[148 241]\n",
            " [131 534]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jJ93gGQ23hpp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_data = pd.read_csv('/content/fcac6286-6db1-4577-ad80-612fb9d36db9_test.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i4NMLpJz32P7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_clean = cln.All_clean(test_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0eFVj_B-9TuM",
        "colab_type": "code",
        "outputId": "9a61cb22-2298-4f69-94f4-e61a4abfb37f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "test_clean.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1153, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5PvNM7Em88CY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pre = cls_clf.test(test_clean)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OEktyhyU-fss",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "A = pd.read_csv('/content/submission_1_svm.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8iCK1qFR-qYI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(A)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}